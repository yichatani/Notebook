# Ideas


## [2024/12/25]

**顺序学习模块：**
   - 在感知模块后增加一个GNN（图神经网络），用物体之间的关系图预测抓取顺序。
   - GNN的输出可以直接作为扩散模型的条件输入。

**推荐训练策略:**
   - 分阶段训练：预训练感知模块 + 端到端微调，确保透明性和适配性。
   - 端到端训练的约束引导：引入辅助损失和对比学习，避免特征学习无效化。

**感知模块：**
   - 输入：场景的RGB和深度图。
   - 输出：场景特征（全局）+ 物体特征（局部），用于指导轨迹生成。

**轨迹扩散模型：**
   - 将感知特征作为条件输入，生成动态环境中的端到端轨迹。

**优化模块：**
   - 通过轨迹平滑、碰撞检测和抓取对齐优化生成路径。

**动态适应与顺序感知：**
   - 实现实时特征更新和抓取顺序预测，提升模型的适应性和效率。

**点云和rgb：**
   - 先只用点云，如果要用rgb也许考虑分别处理RGB和点云，然后在中间层融合。



## [2024/12/8]

**diffusion policy 原版是<u>*离线训练*</u>的。**

**双层diffusion：**
   - 高层diffusion
   - 低层diffusion

**点云分割模块：**
   - 得到物体分割点云信息。
   - 处理得到物体的几何信息。

**高层diffusion：**
   - 输入：
     - **总场景点云**：描述整个场景的三维空间信息，包含物体和可能的背景（如地面、障碍物等）。
     - **物体的分割点云**：从总场景点云中分割出的每个物体的点云，提供单个物体的几何和位置特征。
     - **每个物体的几何信息**：每个物体的几何信息，包括中心位置、尺寸（长宽高），用于分析抓取成本、遮挡关系等。

   

------
#### **diffusion policy 需要记录的数据**

1. **State（状态）：**

   - 当前机器人的末端位姿（TCP/tool0）：x, y, z, (x, y, z, w)	# 用四元数表示	#直接通过RTDE得到。
   - 当前机器人的关节角度。	#RTDE
   - 当前机器人关节速度。
   - 环境观察image。
   - 环境观察depth。
   - 环境观察pointcloud

2. **Action（动作）：**

   - 目标姿态（target pose）定义为下一步运动目标。或者关节角度。
   - 下一步关节速度。 # 暂时不确定怎么用
   - 夹爪开闭角度。

------




#### **思考轨迹：无法使用奖励函数的在线微调方法**

1. **初步理解：奖励函数设计的限制**
   - 在一些复杂的任务中，很难通过简单的规则定义奖励函数。例如，如何量化“抓取动作的优雅程度”或“是否正确完成复杂组装”。
   - 奖励函数的设计需要精准描述任务目标，但不完善的奖励可能导致模型学习方向偏离。
2. **深度探索：无奖励情况下的替代策略**
   - 如果不能明确奖励，可以转向以下方法：
     - **基于人类反馈的优化（Human-in-the-Loop Learning）。**
     - **利用自监督或模仿学习。**
     - **使用环境中的“约束”作为隐式奖励信号。**
3. **跨域联系：从强化学习到无监督学习的过渡**
   - 在强化学习中，奖励函数定义目标；而在无监督或自监督学习中，模型依赖数据结构或目标近似（如分布相似性）进行优化。
   - 在一些场景中，可以通过“生成质量”和“任务完成度”的代理指标（如距离目标的余差）替代明确的奖励。
4. **假设生成：Diffusion Policy 的潜在调整方式**
   - Diffusion Policy 的生成特性可以利用去噪质量或生成轨迹的自一致性进行调整，而不依赖明确奖励。

------

#### **具体的在线微调方法**

**1. 基于人类反馈（Human-in-the-Loop）**

- 方法：

  利用人类直接评价模型生成的动作序列质量，并将评价转化为监督信号，用于调整 Diffusion Policy。

  - 示例：机器人抓取任务中，人类可以标记“成功”或“失败”动作。

- 实现：

  - 将人类反馈作为标签，训练一个辅助网络，将生成动作的好坏评分转化为优化信号。
  - 在生成的 Diffusion Policy 中，将评分结果作为额外条件引导生成。

- **优点：** 不需要明确的奖励函数。

- **挑战：** 人类标注可能耗时，且需要一定规模的标注数据。

------

**2. 自监督优化（Self-Supervised Learning）**

- 方法：

  使用模型生成结果的内部一致性或约束来优化策略。例如：

  - 通过对生成动作序列的物理约束验证模型行为的合理性。
  - 生成的动作是否符合任务的物理规律或环境约束。

- 实现：

  - **轨迹平滑性：** 优化生成动作序列的平滑程度（避免不必要的震荡或不连续性）。
  - **任务对称性：** 若任务有对称特性（如抓取目标可从不同角度进行），对称性约束可以作为优化目标。

- **优点：** 依赖任务固有特性，无需外部标注或奖励。

- **挑战：** 不适用于完全未知或无明确约束的任务。

------

**3. 模仿学习（Imitation Learning）**

- **方法：**
  在特定任务中，利用示例轨迹（如人类专家操作）进行策略微调。
- 实现：
  - **行为克隆（Behavior Cloning）：**
    使用专家轨迹直接监督生成动作序列，调整模型输出。
  - **逆强化学习（Inverse Reinforcement Learning）：**
    利用专家数据推导隐式奖励函数，通过该奖励函数优化策略。
- **优点：** 适合明确任务场景。
- **挑战：** 需要高质量的专家示例数据，且专家行为未必完全适应当前环境。

------

**4. 基于 Diffusion Policy 自适应生成**

- **方法：**
  利用 Diffusion Policy 的生成特性，在实际任务中动态调整去噪过程。
- 实现：
  - 生成的动作序列经过环境执行后，捕获执行结果并进行评价（如与目标状态的相似度）。
  - 将执行结果映射为 Diffusion Policy 的噪声调整条件，改进后续生成。
- **优点：** 可以结合模型的不确定性进行动态优化。
- **挑战：** 执行评价的定义可能较难。

------

**结合场景优化 Diffusion Policy 的建议**

1. **通过示例与真实数据结合：**
   - 将真实数据中的动作示例用于微调，结合 Diffusion Policy 的生成能力，让模型生成接近真实分布的动作。
2. **利用多样性作为隐式奖励：**
   - 强化模型生成不同动作序列的能力，并通过后验概率验证这些序列的完成度。
3. **动态探索与在线校准：**
   - 模型在探索过程中生成动作，同时利用任务的约束条件（如能量消耗最小、轨迹光滑）进行实时调整。
